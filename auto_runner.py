# auto_runner.py - å®šæ—¶å®šç‚¹æ‰¹é‡ç§‘ç ”ç½‘ç«™çˆ¬å–å®Œæ•´ä»£ç ï¼ˆç›´æ¥é›†æˆ research-spider ä½¿ç”¨ï¼‰

import pandas as pd
from spider import runner
import time
from datetime import datetime


def run_crawl_task():
    df_sites = pd.read_csv("config/sites_to_crawl.csv")
    for idx, row in df_sites.iterrows():
        if int(row['enable']) != 1:
            continue
        url = row['url']
        print(f"\nğŸš€ [{datetime.now()}] Starting crawl for {row['site_name']} - {url}")
        try:
            runner.crawl_site(url)
        except Exception as e:
            print(f"âŒ Error while crawling {row['site_name']}: {e}")


if __name__ == "__main__":
    while True:
        now = datetime.now()
        # æ¯å¤©å‡Œæ™¨ 03:00 è‡ªåŠ¨æ‰§è¡Œ
        if now.hour == 3 and now.minute == 0:
            run_crawl_task()
            print("âœ… All sites crawled, sleeping until next day.")
            time.sleep(60)  # é˜²æŠ–ï¼Œé¿å…ä¸€åˆ†é’Ÿå†…é‡å¤æ‰§è¡Œ
        else:
            time.sleep(30)  # æ¯ 30 ç§’æ£€æŸ¥ä¸€æ¬¡æ—¶é—´