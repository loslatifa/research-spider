# main.py - é›†æˆ robots.txt è‡ªåŠ¨æ£€æŸ¥çš„ç§‘ç ”é€šç”¨çˆ¬è™«å…¥å£

from spider import runner, validator
from spider import robots_checker
import os

if __name__ == "__main__":
    print("\nğŸš€ Research Spider Framework: Universal Crawler with robots.txt check")
    #url = input("Enter the URL to crawl (e.g., https://example.com): ").strip()
    url = 'http://quotes.toscrape.com/'
    if not url.startswith("http"):
        url = "https://" + url

    # è‡ªåŠ¨æ£€æŸ¥ robots.txt
    allowed = robots_checker.can_fetch_url(url)
    if not allowed:
        print("âŒ Crawling aborted due to robots.txt disallowing this URL.")
        exit()

    # Crawl and save data
    runner.crawl_site(url)

    # Validate the generated CSV
    output_csv = os.path.join("data", "result.csv")
    validator.validate_csv(output_csv)

    print("\nğŸ‰ Crawling and validation completed. Data ready for analysis.")
