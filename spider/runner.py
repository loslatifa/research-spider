# spider/runner.py - å®Œæ•´è‡ªåŠ¨åœæ­¢åˆ†é¡µæŠ“å–ç‰ˆï¼ˆæŠ“å–å®Œæ‰€æœ‰é¡µè‡ªåŠ¨åœæ­¢ï¼‰

import os
import pandas as pd
from . import fetcher, parser, utils
from urllib.parse import urlparse, urljoin

os.makedirs("data", exist_ok=True)

def crawl_site(base_url):
    """
    çˆ¬å– base_urlï¼Œè‡ªåŠ¨é€’å½’åˆ†é¡µï¼Œç›´åˆ°æ— ä¸‹ä¸€é¡µè‡ªåŠ¨åœæ­¢ï¼Œä¿å­˜ CSV å‘½å result_[åŸŸå]_[æ—¥æœŸ].csv
    """
    all_data = []
    to_visit = [base_url]
    visited = set()

    while to_visit:
        url = to_visit.pop(0)
        if url in visited:
            continue

        print(f"ğŸŒ Crawling: {url}")
        html = fetcher.fetch_url(url)
        if html is None:
            print(f"âš ï¸ Skipping {url} due to fetch error.")
            continue

        page_data = parser.parse_quotes_page(html, url=url)
        if page_data:
            all_data.extend(page_data)
            print(f"âœ… Extracted {len(page_data)} items from {url}")
        else:
            print(f"âš ï¸ No data extracted from {url}")

        visited.add(url)

        # è‡ªåŠ¨æ£€æµ‹ä¸‹ä¸€é¡µå¹¶åŠ å…¥é˜Ÿåˆ—
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')
        next_page = soup.find('li', class_='next')
        if next_page:
            next_url = urljoin(url, next_page.a['href'])
            if next_url not in visited and next_url not in to_visit:
                to_visit.append(next_url)
        else:
            print("âœ… No next page detected. Stopping crawl.")

        utils.random_sleep()

    if all_data:
        df = pd.DataFrame(all_data)
        domain = urlparse(base_url).netloc.replace('.', '_')
        date_str = utils.get_timestamp(date_only=True)
        csv_filename = f"result_{domain}_{date_str}.csv"
        csv_path = os.path.join("data", csv_filename)
        df.to_csv(csv_path, index=False)
        print(f"\nâœ… Crawl completed. Data saved to {csv_path}, total {len(df)} rows.")
    else:
        print("âŒ No data was collected during this crawl.")