# runner.py - å®Œæ•´è‡ªåŠ¨è¯†åˆ«è§£æå™¨ã€ç¨³å®šåˆ†é¡µæŠ“å–å¹¶æ¥å…¥å¯è§†åŒ–æµæ°´çº¿ç‰ˆæœ¬

import os
import pandas as pd
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import requests
import json

from spider import fetcher, parser, utils

os.makedirs("data", exist_ok=True)

def select_parser(url):
    """
    æ ¹æ® URL è‡ªåŠ¨é€‰æ‹©å¯¹åº”è§£æå‡½æ•°ã€‚
    """
    domain = urlparse(url).netloc
    if "quotes.toscrape.com" in domain:
        return parser.parse_quotes_page
    elif "arxiv.org" in domain:
        return parser.parse_arxiv_page
    elif "pubmed.ncbi.nlm.nih.gov" in domain:
        return parser.parse_pubmed_page
    elif "doaj.org" in domain:
        return parser.parse_doaj_page
    elif "ieeexplore.ieee.org" in domain:
        return parser.parse_ieee_page
    elif "api.openalex.org" in domain:
        return parser.parse_openalex_json
    else:
        print(f"âŒ No parser available for {domain}")
        return None

def crawl_site(base_url):
    """
    è‡ªåŠ¨åˆ†é¡µæŠ“å– + è‡ªåŠ¨è§£æ + è‡ªåŠ¨ CSV ä¿å­˜ + è‡ªåŠ¨å¯è§†åŒ–æµæ°´çº¿ã€‚
    """
    parse_function = select_parser(base_url)
    if parse_function is None:
        print("âŒ No available parser, aborting crawl.")
        return

    all_data = []
    visited = set()
    to_visit = [base_url]

    while to_visit:
        url = to_visit.pop(0)
        if url in visited:
            continue

        print(f"ğŸŒ Crawling: {url}")

        # ç‰¹æ®Šå¤„ç† OpenAlex JSON API æŠ“å–
        if "api.openalex.org" in urlparse(url).netloc:
            response = requests.get(url)
            if response.status_code != 200:
                print(f"âŒ Failed to fetch OpenAlex API, status code: {response.status_code}")
                break
            json_data = response.json()
            page_data = parse_function(json_data, url=url)
            if page_data:
                all_data.extend(page_data)
                print(f"âœ… Extracted {len(page_data)} items from OpenAlex API")
            else:
                print("âš ï¸ No data extracted from OpenAlex API.")

            next_cursor = json_data.get('meta', {}).get('next_cursor')
            if next_cursor:
                next_url = url.split('?')[0] + '?' + '&'.join(
                    [kv for kv in url.split('?')[1].split('&') if not kv.startswith('cursor=')]
                ) + f"&cursor={next_cursor}"
                if next_url not in visited and next_url not in to_visit:
                    to_visit.append(next_url)
            else:
                print("âœ… No next cursor detected, stopping.")
                break

        else:
            html = fetcher.fetch_url(url)
            if html is None:
                print(f"âš ï¸ Skipping {url} due to fetch error.")
                continue

            page_data = parse_function(html, url=url)
            if page_data:
                all_data.extend(page_data)
                print(f"âœ… Extracted {len(page_data)} items from {url}")
            else:
                print(f"âš ï¸ No data extracted from {url}")

            soup = BeautifulSoup(html, 'html.parser')
            next_page = soup.find('li', class_='next')
            if next_page:
                next_url = urljoin(url, next_page.a['href'])
                if next_url not in visited and next_url not in to_visit:
                    to_visit.append(next_url)
            else:
                print("âœ… No next page detected, stopping crawl.")

            utils.random_sleep()

        visited.add(url)

    # ä¿å­˜ CSV
    if all_data:
        df = pd.DataFrame(all_data)
        domain = urlparse(base_url).netloc.replace('.', '_')
        date_str = utils.get_timestamp(date_only=True)
        csv_filename = f"result_{domain}_{date_str}.csv"
        csv_path = os.path.join("data", csv_filename)
        df.to_csv(csv_path, index=False)
        print(f"\nâœ… Crawl completed. Data saved to {csv_path}, total {len(df)} rows.")

        # è‡ªåŠ¨æ¥å…¥å¯è§†åŒ–æµæ°´çº¿
        try:
            import analyze_data
            analyze_data.analyze_csv(csv_path)
            print("âœ… Visualization pipeline completed.")
        except Exception as e:
            print(f"âš ï¸ Visualization pipeline failed: {e}")

    else:
        print("âŒ No data was collected during this crawl.")

if __name__ == "__main__":
    url = input("Enter the URL to crawl (e.g., https://example.com): ").strip()
    crawl_site(url)